import os
import hashlib
import re
from datetime import datetime

# Root Directory (iCloud Mac default)
# Root Directory (iCloud Mac default) # TODO: Make cross-platform for Windows
ROOT_DIR = os.path.join(os.path.expanduser("~"), "Library", "Mobile Documents", "com~apple~CloudDocs", "ç ”ç©¶å·¥ä½œæµ")
if not os.path.exists(ROOT_DIR): # Fallback for Windows/Standard
    ROOT_DIR = os.path.join(os.path.expanduser("~"), "Desktop", "ç ”ç©¶å·¥ä½œæµ")

REPORT_PATH = os.path.join(os.path.dirname(__file__), "..", "PURGE_LIST.md")

# JUNK Patterns (Naming Fog Variations)
JUNK_PATTERNS = [
    r"\.[rt]\s?[0-9]+",       # .r 5, .t 12, .r45
    r"\.rea(son)?(\s?[0-9]+)?", # .rea, .reason, .rea 5
    r"\.bak$",                # .bak files
    r" \(å‰¯æœ¬ [0-9]+\)",      # iCloud Duplicates
]

def get_file_hash(path):
    """Simple MD5 hash for duplicate detection."""
    try:
        hasher = hashlib.md5()
        with open(path, 'rb') as f:
            buf = f.read(65536)
            while len(buf) > 0:
                hasher.update(buf)
                buf = f.read(65536)
        return hasher.hexdigest()
    except:
        return None

def scan():
    print(f"ğŸ”¦ Starting Surgical Scan in: {ROOT_DIR}...")
    
    junk_list = []
    dupe_map = {} # hash -> list of paths
    safe_count = 0
    
    for root, dirs, files in os.walk(ROOT_DIR):
        # Ignore engine-internal folders (already safe)
        if "_QUARANTINE_" in root or ".gemini" in root:
            continue
            
        for file in files:
            file_path = os.path.join(root, file)
            is_junk = False
            
            # 1. Pattern Matching (Known Junk)
            for pattern in JUNK_PATTERNS:
                if re.search(pattern, file, re.IGNORECASE):
                    junk_list.append((file_path, "Naming Relic (Naming Loop Variant)"))
                    is_junk = True
                    break
            
            if is_junk: continue
            
            # 2. Duplicate Detection (The "Hidden" Camouflage)
            f_hash = get_file_hash(file_path)
            if f_hash:
                if f_hash not in dupe_map:
                    dupe_map[f_hash] = []
                dupe_map[f_hash].append(file_path)
            
            safe_count += 1

    # Filter actual duplicates
    actual_dupes = {h: paths for h, paths in dupe_map.items() if len(paths) > 1}
    
    # Generate Report
    with open(REPORT_PATH, 'w') as f:
        f.write(f"# ğŸ§¹ FlashSquirrel: å¤–ç§‘æ‰‹è¡“æŸ¥æ®ºåå–® (V17 Audit)\n\n")
        f.write(f"> ç”Ÿæˆæ™‚é–“: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n\n")
        
        f.write(f"## ğŸ“Š æƒææ¦‚è¦½\n")
        f.write(f"* æ‰¾åˆ°é¡¯æ€§åƒåœ¾: {len(junk_list)} å€‹\n")
        f.write(f"* æ‰¾åˆ°å…§å®¹é‡è¤‡çµ„: {len(actual_dupes)} çµ„\n")
        f.write(f"* æ­£å¸¸æ•¸æ“šé‡: {safe_count} å€‹\n\n")
        
        f.write(f"## ğŸ”´ å»ºè­°æ¸…ç†ï¼šé¡¯æ€§åƒåœ¾ (å¾Œç¶´æ®˜ç•™)\n")
        f.write(f"é€™äº›æª”æ¡ˆå¸¶æœ‰ `.t`ã€`.r`ã€`.rea` ç­‰éŒ¯èª¤å¾Œç¶´ã€‚\n\n")
        for path, reason in junk_list[:100]: # Limit report size
            rel_path = os.path.relpath(path, ROOT_DIR)
            f.write(f"- [ ] `{rel_path}` ({reason})\n")
        if len(junk_list) > 100:
            f.write(f"- ... åŠå…¶ä»– {len(junk_list)-100} å€‹æª”æ¡ˆ\n")
            
        f.write(f"\n## ğŸŸ¡ ç–‘æ…®æ¸…å–®ï¼šå…§å®¹é‡è¤‡ (å½±åˆ†èº«é‘‘å®š)\n")
        f.write(f"é€™äº›æª”æ¡ˆåå­—ã€Œçœ‹èµ·ä¾†å¾ˆæ­£ç¶“ã€ï¼Œä½†å…§å®¹å“ˆå¸Œå€¼ 100% ä¸€è‡´ã€‚å»ºè­°ä¿ç•™ä¸€å€‹ï¼Œåˆªé™¤å…¶é¤˜ã€‚\n\n")
        for h, paths in list(actual_dupes.items())[:50]:
            f.write(f"### çµ„åˆ¥ {h[:8]}\n")
            for p in paths:
                rel_path = os.path.relpath(p, ROOT_DIR)
                f.write(f"- [ ] `{rel_path}`\n")
            f.write("\n")
            
    print(f"âœ… Report generated at: {REPORT_PATH}")

if __name__ == "__main__":
    scan()
